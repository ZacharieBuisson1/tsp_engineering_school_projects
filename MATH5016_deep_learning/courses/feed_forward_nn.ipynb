{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkorange> Logistic regression & Feed forward neural networks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings for better clarity (may not be the best thing to do)...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import activations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras import backend as K\n",
    "# ignore warnings for better clarity (may not be the best thing to do)...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.datasets import cifar100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "The dataset contains 60000 32x32x3 colour images divided in 100 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "num_classes = 100\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "# cifar100 data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "print('input dimension:',x_train.shape[1::])\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the input data to obtain entries in (0,1)\n",
    "x_train = x_train/255\n",
    "x_test  = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display one input data at random\n",
    "plt.imshow(x_train[randint(0, x_train.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkred> Softmax regression </font>\n",
    "Logistic regression can be extended to classify data in more than two groups. Softmax regression provides a model for the probability that an input $x$ is associated with each group.  It is assumed that the probability to belong to the class $k\\in\\{1,\\ldots,M\\}$ can be expressed by \n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = k| X) = \\frac{\\exp(\\langle w_k,X \\rangle + b_k)}{\\sum_{\\ell=1}^{M}\\exp(\\langle w_\\ell,X \\rangle + b_\\ell)} = p_k(X)\\,,\n",
    "\\end{equation*}\n",
    "where $w_\\ell \\in \\mathbb{R}^d$ and $b_\\ell$  are model `weights` and `intercepts` for each class.\n",
    "\n",
    "\n",
    "To estimate these unknown parameters, a maximum likelihood approach is used as in the logistic regression setting. In this case, the loss function is given by the negative log-likelihood (see also the section on gradient based method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model prone to add layers sequentially\n",
    "model = Sequential()\n",
    "# flatten the data replaces 32 * 32 * 3 matrices by a 3072 dimensional vector\n",
    "# This is always necessary before a fully-connected layer (Dense object)\n",
    "model.add(Flatten(input_shape=input_shape, name='flatten'))\n",
    "# add one dense (fully connected layer) with softmax activation function\n",
    "# As it is the first layer, the input size is mandatory\n",
    "model.add(Dense(num_classes, activation='softmax', name='dense_softmax'))\n",
    "\n",
    "# \"compile\" this model, \n",
    "model.compile(\n",
    "    # specify the loss as the cross-entropy i.e. the negative loglikelihood.\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    # choose the gradient based method to estimate the parameters\n",
    "    # see https://keras.io/optimizers/ to have an overview of the different options\n",
    "    # see also section 2 on gradient based methods.\n",
    "    optimizer=keras.optimizers.Adagrad(),\n",
    "    # metric to monitor on the test data\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of data used for each update of the parameter (each gradient computation)\n",
    "batch_size = 64\n",
    "# number of times data are scanned\n",
    "epochs = 50\n",
    "# train the model, i.e. estimate unknown parameters by minimizing the loss function using a gradient descent algorithm (here Adagrad).\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(history.epoch, history.history['acc'], lw=1, label='Train')\n",
    "plt.plot(history.epoch, history.history['val_acc'], lw=1, label='Test')\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Accuracy of softmax regression', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Feed-Forward Neural Network (FFNN) or multilayer Perceptron (MLP)</font>\n",
    "The softmax regression of the previous section is a linear model, with 307300 parameters.  It might be too simple for our classification task.  The idea underlying neural networks is to have successive \"neurons\" performing a linear transformation of the input data (depending on a weight matrix and a bias vector) followed by an activation function to design more flexible models with additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph for a fully connected feed-forward neural network with one hidden layer \n",
    "# with 256 units and a relu activation function. \n",
    "model_ffnn = Sequential()\n",
    "\n",
    "model_ffnn.add(Flatten(input_shape=input_shape))\n",
    "\n",
    "model_ffnn.add(Dense(256, activation='relu'))\n",
    "\n",
    "model_ffnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_ffnn.compile(\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.Adagrad(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_ffnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model the input data $X$ lies in $\\mathbb{R}^d$ with $d = 3072$.\n",
    "\n",
    "A hidden layer is built in $\\mathbb{R}^h$ with $h = 256$.\n",
    "\n",
    "\\begin{align*}\n",
    "z^\\mathrm{hid}(X) &= W^hX+b^h\\,,\\\\\n",
    "h(X) &= \\mathrm{Relu}(z^\\mathrm{hid}(X))\\,.\n",
    "\\end{align*}\n",
    "\n",
    "$W^h\\in\\mathbb{R}^{hxd}$, $b^h\\in\\mathbb{R}^h$, $h(X)\\in\\mathbb{R}^h$ and for all $1\\leqslant j\\leqslant h$, $h(X)_j = \\mathrm{Max}(0,z^\\mathrm{hid}(X)_j)$. \n",
    "\n",
    "The output layer is built in $\\mathbb{R}^M$ with $M = 100$.\n",
    "\n",
    "\\begin{align*}\n",
    "z^\\mathrm{out}(X) &= W^oX+b^o\\,,\\\\\n",
    "f_{\\theta}(X) &= \\mathrm{Softmax}(z^\\mathrm{out}(X))\\,.\n",
    "\\end{align*}\n",
    "\n",
    "$W^o\\in\\mathbb{R}^{Mxh}$, $b^o\\in\\mathbb{R}^o$. \n",
    "\n",
    "$\\theta = (W^h,b^h,W^o,b^o)$.\n",
    "\n",
    "$f_{\\theta}(X)$ is a vector in $\\mathbb{R}^M$ where each entry is the probability that $X$  belongs to the corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "history = model_ffnn.fit(x_train, y_train,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         verbose=1,\n",
    "                         validation_data=(x_test, y_test))\n",
    "score = model_ffnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(history.epoch, history.history['acc'], lw=1, label='Train')\n",
    "plt.plot(history.epoch, history.history['val_acc'], lw=1, label='Test')\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Accuracy of softmax regression', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters is much larger than in the softmax setting while the performance only slightly improves. See next course on `Convolutional networks to provide models more suitable to image data`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Use cross-validation to find the best values of $h$ for instance in $\\{32,64,128,256,512\\}$...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
